{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1123],\n",
       "        [0.1110]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X=torch.rand(size=(2,4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('weight',\n",
       "               tensor([[ 0.1952,  0.1933,  0.3364,  0.2534,  0.1545,  0.2872, -0.1041, -0.0663]])),\n",
       "              ('bias', tensor([-0.2922]))]),\n",
       " OrderedDict([('0.weight',\n",
       "               tensor([[ 0.4259, -0.2544,  0.3114, -0.0191],\n",
       "                       [ 0.2780,  0.3777,  0.2822, -0.2920],\n",
       "                       [-0.0065,  0.2706,  0.1803,  0.2921],\n",
       "                       [-0.0469, -0.3680, -0.2666, -0.2596],\n",
       "                       [ 0.0953,  0.3712, -0.2075, -0.0467],\n",
       "                       [ 0.0141,  0.2735,  0.2935,  0.4018],\n",
       "                       [ 0.3295,  0.3687,  0.3892, -0.1069],\n",
       "                       [-0.3475, -0.0948, -0.1918,  0.1337]])),\n",
       "              ('0.bias',\n",
       "               tensor([-0.4022,  0.2000,  0.2616, -0.0250,  0.2335,  0.1191,  0.1461, -0.0615])),\n",
       "              ('2.weight',\n",
       "               tensor([[ 0.1952,  0.1933,  0.3364,  0.2534,  0.1545,  0.2872, -0.1041, -0.0663]])),\n",
       "              ('2.bias', tensor([-0.2922]))]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict(),net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.2922], requires_grad=True)\n",
      "tensor([-0.2922])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "net[2].weight.grad==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x0000022FCE85E040>\n",
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2922])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net[0].named_parameters())\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                         nn.Linear(8,4),nn.ReLU())\n",
    "def block2():\n",
    "    net=nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())\n",
    "    return net\n",
    "\n",
    "rgnet=nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1071],\n",
       "         [0.1071]], grad_fn=<AddmmBackward0>),\n",
       " tensor([-0.0297,  0.0988, -0.0752,  0.4339,  0.2289, -0.4404, -0.1611, -0.1208]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet(X),rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0098,  0.0114, -0.0095, -0.0040]), tensor(0.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1456, -0.5308, -0.4543, -0.0233]),\n",
       " tensor([[42., 42., 42., 42., 42., 42., 42., 42.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "net[0].weight.data[0],net[2].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9181, -0.0000, -7.4324,  0.0000],\n",
       "        [-0.0000, -0.0000, -7.6489, -0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        print(\"Init\",*[(name,param.shape) for name,param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        m.weight.data*=m.weight.data.abs()>=5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000, -6.4324,  1.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:]+=1\n",
    "net[0].weight.data[0,0]=42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared=nn.Linear(8,8)   # 我们需要给共享层一个名称,以便可以引用它的参数\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                  shared,nn.ReLU(),\n",
    "                  shared,nn.ReLU(),\n",
    "                  nn.Linear(8,1))\n",
    "net(X)\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0]) # 检查参数是否相同\n",
    "net[2].weight.data[0,0]=100\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0]) # 确保它们实际上是同一个对象,而不只是有相同的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net.0.weight': torch.Size([64, 20]),\n",
       " 'net.0.bias': torch.Size([64]),\n",
       " 'net.2.weight': torch.Size([32, 64]),\n",
       " 'net.2.bias': torch.Size([32]),\n",
       " 'linear.weight': torch.Size([16, 32]),\n",
       " 'linear.bias': torch.Size([16])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                               nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera=NestMLP()\n",
    "{k:v.shape for k,v in chimera.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera.state_dict()['net.2.weight'][0][0]==chimera.net[2].weight[0][0],\\\n",
    "chimera.state_dict()['linear.weight'][0][0]==chimera.linear.weight[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练包含共享参数层的多层感知机（以多项式回归为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0  weight.data[0]: tensor([-0.3648,  0.2111, -0.3065, -0.0989])  weight.bias[0]: tensor([0., 0., 0., 0.])\n",
      "layer 1  weight.data[0]: tensor([-0.1947,  0.0518,  0.0532, -0.3742, -0.0641, -0.0398,  0.7691, -0.2661])  weight.bias[0]: tensor([ 0.0072, -0.0011,  0.0171,  0.0041,  0.0020,  0.0342,  0.0062,  0.0091])\n",
      "layer 2  weight.data[0]: tensor([-0.1947,  0.0518,  0.0532, -0.3742, -0.0641, -0.0398,  0.7691, -0.2661])  weight.bias[0]: tensor([ 0.0072, -0.0011,  0.0171,  0.0041,  0.0020,  0.0342,  0.0062,  0.0091])\n",
      "layer 3  weight.data[0]: tensor([-0.7376,  0.2640,  0.1476, -0.5621,  0.2634, -0.9461,  1.1562, -0.2565])  weight.bias[0]: tensor([ 0.0088,  0.0000, -0.0078, -0.0023,  0.0000, -0.0079, -0.1378,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "\n",
    "n_train,n_test=200,100\n",
    "true_w=np.zeros(4)\n",
    "true_w[0:4]=np.array([5,1.2,-3.4,5.6])\n",
    "\n",
    "features=np.random.normal(size=(n_train+n_test,1))\n",
    "np.random.shuffle(features)\n",
    "poly_features=np.power(features,np.arange(4).reshape(1,-1))\n",
    "for i in range(4):\n",
    "    poly_features[:,i]/=math.gamma(i+1)\n",
    "labels=np.dot(poly_features,true_w)\n",
    "labels+=np.random.normal(scale=0.1,size=labels.shape)\n",
    "true_w,features,poly_features,labels=[torch.tensor(x,dtype=torch.float32) for x in [true_w,features,poly_features,labels]]\n",
    "\n",
    "def train(train_features,test_features,train_labels,test_labels,num_epochs=400):\n",
    "    loss=nn.MSELoss()\n",
    "    shared=nn.Linear(8,8,bias=False)\n",
    "    net=nn.Sequential(nn.Linear(4,8,bias=False),nn.ReLU(),\n",
    "                      shared,nn.ReLU(),shared,nn.ReLU(),\n",
    "                      nn.Linear(8,1,bias=False))\n",
    "    batch_size=min(10,train_labels.shape[0])\n",
    "    train_iter=d2l.load_array((train_features,train_labels.reshape(-1,1)),batch_size)\n",
    "    test_iter=d2l.load_array((test_features,test_labels.reshape(-1,1)),batch_size,is_train=False)\n",
    "    trainer=torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "    for epoch in range(num_epochs):\n",
    "        d2l.train_epoch_ch3(net,train_iter,loss,trainer)\n",
    "    for i in [0,2,4,6]: # 结果可以看到共享层参数的值和梯度是相同的\n",
    "        print('layer',i//2,' weight.data[0]:',net[i].weight.data[0],' weight.bias[0]:',net[i].weight.grad[0])\n",
    "\n",
    "train(poly_features[:n_train,:4],poly_features[n_train:,:4],labels[:n_train],labels[n_train:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
